{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d6925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from typing import Union\n",
    "import os\n",
    "\n",
    "max_dataset_size = 200000  # 你定义的最大数据量（可选：用于调试或限制内存）\n",
    "\n",
    "class LCSTS(Dataset):\n",
    "    def __init__(self, data_file, start_idx=0, end_idx=None):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param data_file: JSON 文件路径，包含 list of dict，每个 dict 有 'title' 和 'content'\n",
    "        :param start_idx: 起始索引（用于划分数据集）\n",
    "        :param end_idx: 结束索引\n",
    "        \"\"\"\n",
    "        self.data = self.load_data(data_file, start_idx, end_idx)\n",
    "    \n",
    "    def load_data(self, data_file, start_idx, end_idx):\n",
    "        \"\"\"\n",
    "        加载数据并按索引切片\n",
    "        \"\"\"\n",
    "        # 读取 JSON 文件\n",
    "        if not os.path.exists(data_file):\n",
    "            raise FileNotFoundError(f\"{data_file} not found.\")\n",
    "        \n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)  # 假设是 list of dict\n",
    "\n",
    "        # 确保是列表格式\n",
    "        if isinstance(raw_data, dict):\n",
    "            # 有些数据可能包裹在 {'data': [...]} 中\n",
    "            raw_data = raw_data.get('data', []) if 'data' in raw_data else []\n",
    "        \n",
    "        # 截取指定范围\n",
    "        if end_idx is None:\n",
    "            end_idx = len(raw_data)\n",
    "        \n",
    "        # 应用全局最大限制（可选）\n",
    "        start_idx = min(start_idx, max_dataset_size)\n",
    "        end_idx = min(end_idx, max_dataset_size, len(raw_data))\n",
    "        \n",
    "        # 切片\n",
    "        selected_data = raw_data[start_idx:end_idx]\n",
    "        \n",
    "        # 转为列表，确保索引访问\n",
    "        return selected_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a763f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2000\n",
      "Validation set size: 1000\n",
      "Testing set size: 1000\n",
      "Sample from train set:\n",
      "{'title': '修改后的立法法全文公布', 'content': '新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。'}\n"
     ]
    }
   ],
   "source": [
    "# 假设你的 JSON 文件路径\n",
    "data_file = 'data/lcsts_data.json'\n",
    "\n",
    "# 定义划分范围\n",
    "train_dataset = LCSTS(data_file, start_idx=0,      end_idx=2000)\n",
    "valid_dataset = LCSTS(data_file, start_idx=2000,  end_idx=3000)\n",
    "test_dataset  = LCSTS(data_file, start_idx=3000,  end_idx=4000)  # 注意：你写的是 300000，可能是笔误\n",
    "\n",
    "# 打印信息\n",
    "print(f'Training set size: {len(train_dataset)}')   # 20000\n",
    "print(f'Validation set size: {len(valid_dataset)}') # 10000\n",
    "print(f'Testing set size: {len(test_dataset)}')     # 10000\n",
    "\n",
    "# 示例：查看第一条训练数据\n",
    "print(\"Sample from train set:\")\n",
    "print(next(iter(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02631bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "model_checkpoint = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./models/\")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/\", use_fast=False)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['content'])\n",
    "        batch_targets.append(sample['title'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs, \n",
    "        padding=True, \n",
    "        max_length=max_input_length,\n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets, \n",
    "            padding=True, \n",
    "            max_length=max_target_length,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a806dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[   381,    891,   2116,    838,    261,  76581,   1223,  19185,   5510,\n",
      "         238814,  23418,  36444,  17933,  14841,   2619, 201521,   2037,  76581,\n",
      "           1223,  19185,   5510, 238814,  23418, 148577,  19347,    493,  78727,\n",
      "         124974, 184310,  51838,    306,  17933,  14841,   2619, 127646, 161296,\n",
      "          42964,   2445,  33119,   1083,   5028,  24659, 127974,   1644,  17955,\n",
      "          54093,   3582,   5435,    261,   5144,   2991,  17933,  14841,   2619,\n",
      "           5705,   3480,  30765,   1543,  97806, 146315,  23818,    493, 128334,\n",
      "            306,      1,      0,      0,      0,      0,      0,      0],\n",
      "        [   259,  22014, 215154,    261,  76581,   4938,   3802,  32738,   5705,\n",
      "          24673,   2372,  26979,   5510, 238814,   3017,  17823, 184310,    261,\n",
      "           2037,    591,  76581,   8893,   8369, 102873,  24659,    365, 134986,\n",
      "          49089,  15104,  24673,  42357,   1107,   4462, 177996,    261,   8349,\n",
      "           2037,   2645,   2219,  31568,  49662,  63255,   1322,   7367,  43451,\n",
      "          12348,   3709,  21158,   4462,    306,   2645,   2219, 210552,   8349,\n",
      "         225282,  29492, 169163,  13746,   1107, 206625,    493,  19347, 109277,\n",
      "           4462, 177996,   2811,   4938,   3802,  32738,    306,      1],\n",
      "        [   259,  92230,   5039,  28290,  57479,  27341,   8659,    261,  84864,\n",
      "          15727,  55161,   1083,  10428,  18105,   6380,  25293,    306, 213796,\n",
      "           1637,  18105, 118350,   5836,  22014,  84973,    267, 119579,  76909,\n",
      "         157668, 177222,  10458,   2092,   8922,  29003,  51385, 121311,    261,\n",
      "         107649,    660,   2688,   2884,  70506,   6651,   1597,  22469,    306,\n",
      "           2037,   9790,  46296,   7321,   2688,  71625,    261,  83455, 118350,\n",
      "         129617,  46912,   3913,  46296,  17953, 132905,   4329,      1,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   259,  22848,   4066,    292,  22848,  61584,    292, 231021,    292,\n",
      "         211228,   4669,  89441,  58202, 180984,   2518,  28916,  96426,  33073,\n",
      "          63783,  38286,   1193, 193886,  10220, 176263, 195517,  66286,  93758,\n",
      "          14903, 217499, 149709,  22848,  61584,   6825, 105301,   5857,  22061,\n",
      "           5857,   4898,  22695,  31588,  45606,  30014,   5496, 132701,  24213,\n",
      "          10220, 176263,   3582,  89703,    261, 132701,  24213,  17955,  12561,\n",
      "          84273,   1193,   5742,  36808,    292,  77567,    493,  69740, 211171,\n",
      "          72802,   3801,  22136,   6825, 105301,    306,      1,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'decoder_input_ids': tensor([[     0,    259,  42964,   2445,  33119,    267,  50611,   5510, 238814,\n",
      "           3017,  17823, 162293,   9893,   2991,  17933,  14841,   2619, 106382,\n",
      "          11364,  19185],\n",
      "        [     0,    259,   4938,   3802,  32738,    591, 102873,  24659,    365,\n",
      "          11022,   6450, 177222,   7367,  43451,  21158,   4462, 103218,  49662,\n",
      "          10139,      1],\n",
      "        [     0,    259,   5039,  28290,  27341,   8659, 200127, 119579, 132905,\n",
      "           3139, 118350,  92704,  33782,  65782,   8227,   5742,  46912, 122097,\n",
      "              1,      0],\n",
      "        [     0,    259,  22848,  61584,    267, 132701,  24213,  10220, 176263,\n",
      "          17955,   5742,  36808,  77567,  14952,  61584,   4222,   3801,  22136,\n",
      "           6825, 105301]]), 'labels': tensor([[   259,  42964,   2445,  33119,    267,  50611,   5510, 238814,   3017,\n",
      "          17823, 162293,   9893,   2991,  17933,  14841,   2619, 106382,  11364,\n",
      "          19185,      1],\n",
      "        [   259,   4938,   3802,  32738,    591, 102873,  24659,    365,  11022,\n",
      "           6450, 177222,   7367,  43451,  21158,   4462, 103218,  49662,  10139,\n",
      "              1,   -100],\n",
      "        [   259,   5039,  28290,  27341,   8659, 200127, 119579, 132905,   3139,\n",
      "         118350,  92704,  33782,  65782,   8227,   5742,  46912, 122097,      1,\n",
      "           -100,   -100],\n",
      "        [   259,  22848,  61584,    267, 132701,  24213,  10220, 176263,  17955,\n",
      "           5742,  36808,  77567,  14952,  61584,   4222,   3801,  22136,   6825,\n",
      "         105301,      1]])})\n",
      "batch shape: {'input_ids': torch.Size([4, 71]), 'attention_mask': torch.Size([4, 71]), 'decoder_input_ids': torch.Size([4, 20]), 'labels': torch.Size([4, 20])}\n",
      "{'input_ids': tensor([[   381,    891,   2116,    838,    261,  76581,   1223,  19185,   5510,\n",
      "         238814,  23418,  36444,  17933,  14841,   2619, 201521,   2037,  76581,\n",
      "           1223,  19185,   5510, 238814,  23418, 148577,  19347,    493,  78727,\n",
      "         124974, 184310,  51838,    306,  17933,  14841,   2619, 127646, 161296,\n",
      "          42964,   2445,  33119,   1083,   5028,  24659, 127974,   1644,  17955,\n",
      "          54093,   3582,   5435,    261,   5144,   2991,  17933,  14841,   2619,\n",
      "           5705,   3480,  30765,   1543,  97806, 146315,  23818,    493, 128334,\n",
      "            306,      1,      0,      0,      0,      0,      0,      0],\n",
      "        [   259,  22014, 215154,    261,  76581,   4938,   3802,  32738,   5705,\n",
      "          24673,   2372,  26979,   5510, 238814,   3017,  17823, 184310,    261,\n",
      "           2037,    591,  76581,   8893,   8369, 102873,  24659,    365, 134986,\n",
      "          49089,  15104,  24673,  42357,   1107,   4462, 177996,    261,   8349,\n",
      "           2037,   2645,   2219,  31568,  49662,  63255,   1322,   7367,  43451,\n",
      "          12348,   3709,  21158,   4462,    306,   2645,   2219, 210552,   8349,\n",
      "         225282,  29492, 169163,  13746,   1107, 206625,    493,  19347, 109277,\n",
      "           4462, 177996,   2811,   4938,   3802,  32738,    306,      1],\n",
      "        [   259,  92230,   5039,  28290,  57479,  27341,   8659,    261,  84864,\n",
      "          15727,  55161,   1083,  10428,  18105,   6380,  25293,    306, 213796,\n",
      "           1637,  18105, 118350,   5836,  22014,  84973,    267, 119579,  76909,\n",
      "         157668, 177222,  10458,   2092,   8922,  29003,  51385, 121311,    261,\n",
      "         107649,    660,   2688,   2884,  70506,   6651,   1597,  22469,    306,\n",
      "           2037,   9790,  46296,   7321,   2688,  71625,    261,  83455, 118350,\n",
      "         129617,  46912,   3913,  46296,  17953, 132905,   4329,      1,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   259,  22848,   4066,    292,  22848,  61584,    292, 231021,    292,\n",
      "         211228,   4669,  89441,  58202, 180984,   2518,  28916,  96426,  33073,\n",
      "          63783,  38286,   1193, 193886,  10220, 176263, 195517,  66286,  93758,\n",
      "          14903, 217499, 149709,  22848,  61584,   6825, 105301,   5857,  22061,\n",
      "           5857,   4898,  22695,  31588,  45606,  30014,   5496, 132701,  24213,\n",
      "          10220, 176263,   3582,  89703,    261, 132701,  24213,  17955,  12561,\n",
      "          84273,   1193,   5742,  36808,    292,  77567,    493,  69740, 211171,\n",
      "          72802,   3801,  22136,   6825, 105301,    306,      1,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'decoder_input_ids': tensor([[     0,    259,  42964,   2445,  33119,    267,  50611,   5510, 238814,\n",
      "           3017,  17823, 162293,   9893,   2991,  17933,  14841,   2619, 106382,\n",
      "          11364,  19185],\n",
      "        [     0,    259,   4938,   3802,  32738,    591, 102873,  24659,    365,\n",
      "          11022,   6450, 177222,   7367,  43451,  21158,   4462, 103218,  49662,\n",
      "          10139,      1],\n",
      "        [     0,    259,   5039,  28290,  27341,   8659, 200127, 119579, 132905,\n",
      "           3139, 118350,  92704,  33782,  65782,   8227,   5742,  46912, 122097,\n",
      "              1,      0],\n",
      "        [     0,    259,  22848,  61584,    267, 132701,  24213,  10220, 176263,\n",
      "          17955,   5742,  36808,  77567,  14952,  61584,   4222,   3801,  22136,\n",
      "           6825, 105301]]), 'labels': tensor([[   259,  42964,   2445,  33119,    267,  50611,   5510, 238814,   3017,\n",
      "          17823, 162293,   9893,   2991,  17933,  14841,   2619, 106382,  11364,\n",
      "          19185,      1],\n",
      "        [   259,   4938,   3802,  32738,    591, 102873,  24659,    365,  11022,\n",
      "           6450, 177222,   7367,  43451,  21158,   4462, 103218,  49662,  10139,\n",
      "              1,   -100],\n",
      "        [   259,   5039,  28290,  27341,   8659, 200127, 119579, 132905,   3139,\n",
      "         118350,  92704,  33782,  65782,   8227,   5742,  46912, 122097,      1,\n",
      "           -100,   -100],\n",
      "        [   259,  22848,  61584,    267, 132701,  24213,  10220, 176263,  17955,\n",
      "           5742,  36808,  77567,  14952,  61584,   4222,   3801,  22136,   6825,\n",
      "         105301,      1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clione/miniconda3/envs/learn_transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f94055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d56bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2,\n",
    "            ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [' '.join(pred.strip()) for pred in decoded_preds]\n",
    "        labels += [' '.join(label.strip()) for label in decoded_labels]\n",
    "    scores = rouge.get_scores(hyps=preds, refs=labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    result['avg'] = np.mean(list(result.values()))\n",
    "    print(f\"Rouge1: {result['rouge-1']:>0.2f} Rouge2: {result['rouge-2']:>0.2f} RougeL: {result['rouge-l']:>0.2f}\\n\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e5f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:37<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 26.07 Rouge2: 13.80 RougeL: 23.28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': 26.06907931080389,\n",
       " 'rouge-2': 13.800633335270968,\n",
       " 'rouge-l': 23.278877761486054,\n",
       " 'avg': np.float64(21.04953013585364)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估下基准性能\n",
    "test_loop(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf3647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.000000:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_avg_rouge = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_rouge = test_loop(valid_dataloader, model)\n",
    "    print(valid_rouge)\n",
    "    rouge_avg = valid_rouge['avg']\n",
    "    if rouge_avg > best_avg_rouge:\n",
    "        best_avg_rouge = rouge_avg\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_rouge_{rouge_avg:0.4f}_model_weights.bin')\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
